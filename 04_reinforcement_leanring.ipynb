{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym # openai gym\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from data import get_data, p2df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate some fake data\n",
    "# dates = pd.date_range(start='2022-01-01', end='2023-01-31', freq='D')\n",
    "# open_prices = np.random.randint(low=100, high=150, size=len(dates))\n",
    "# high_prices = open_prices + np.random.randint(low=0, high=50, size=len(dates))\n",
    "# low_prices = open_prices - np.random.randint(low=0, high=50, size=len(dates))\n",
    "# close_prices = open_prices + np.random.randint(low=-10, high=10, size=len(dates))\n",
    "# volume = np.random.randint(low=1000, high=10000, size=len(dates))\n",
    "\n",
    "# # Create a DataFrame with the data\n",
    "# df = pd.DataFrame({\n",
    "#     'date': dates,\n",
    "#     'open': open_prices,\n",
    "#     'high': high_prices,\n",
    "#     'low': low_prices,\n",
    "#     'close': close_prices,\n",
    "#     'volume': volume\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THIS WE WILL USE THE POLYGON API\n",
    "load_dotenv()\n",
    "POLYGON_API_KEY = os.getenv('POLYGON_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load some data, we will use TSLA\n",
    "data = get_data(POLYGON_API_KEY, ticker=\"TSLA\", multiplier=1, timespan=\"hour\", from_=\"2021-01-09\", to=\"2023-02-10\", limit=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/BTC-2017min.csv')\n",
    "\n",
    "# # convert date to datetime\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# # lets sort the data with date and reset index\n",
    "# df = df.sort_values('date')\n",
    "# df = df.reset_index(drop=True)\n",
    "\n",
    "# # grabbing the values we care about\n",
    "# df = df[['open', 'high', 'low', 'close', 'Volume USD']]\n",
    "\n",
    "# # rename Volume USD to volume\n",
    "# df = df.rename(columns={'Volume USD': 'volume'})\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our data into a dataframe\n",
    "df = p2df(data, convert_timestamp=True) # convert timestamp to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['open', 'high', 'low', 'close', 'volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, cols2norm=['open', 'high', 'low', 'volume'], target='close'):\n",
    "    target = data[target].values # not normalized\n",
    "    data_2_normalize = data[cols2norm].values\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler = scaler.fit(data_2_normalize)\n",
    "    data_normalized = scaler.transform(data_2_normalize)\n",
    "    return data_normalized, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnvironment(gym.Env):\n",
    "    def __init__(self, data, target, initial_capital=100000, start_date=0, end_date=None):\n",
    "        # initialize variables\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.initial_capital = initial_capital\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date if end_date is not None else len(data) - 1\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # reset variables\n",
    "        self.current_date = self.start_date\n",
    "        self.current_price = self.target[self.current_date]\n",
    "        self.current_capital = self.initial_capital\n",
    "        self.current_shares = 0\n",
    "        self.done = False\n",
    "        self.history = []\n",
    "        self.profit_losses = []\n",
    "        self.purchase_price = None\n",
    "\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action, buy_fraction=0):\n",
    "        assert action in [0, 1, 2]\n",
    "\n",
    "        # Simulate price movement based on historical data\n",
    "        if self.current_date < self.end_date:\n",
    "            self.current_date += 1\n",
    "            self.current_price = self.target[self.current_date]\n",
    "\n",
    "        # Calculate the reward based on the action taken\n",
    "        reward = 0\n",
    "        if action == 0:  # Buy\n",
    "            if buy_fraction > 0:\n",
    "                buy_amount = self.current_capital * buy_fraction\n",
    "                buy_shares = int(buy_amount // self.current_price)\n",
    "                if buy_shares > 0 and buy_shares * self.current_price <= self.current_capital:\n",
    "                    self.purchase_price = self.current_price  # record purchase price\n",
    "                    self.current_shares += buy_shares\n",
    "                    self.current_capital -= buy_shares * self.current_price\n",
    "        elif action == 1:  # Sell\n",
    "            if self.current_shares > 0:\n",
    "                profit_loss = self.current_shares * (self.current_price - self.purchase_price)\n",
    "                if profit_loss >= 0:\n",
    "                    reward = profit_loss / self.purchase_price  # positive reward for profit\n",
    "                else:\n",
    "                    reward = profit_loss / self.purchase_price * 2  # negative reward for loss NOTE: This penalizes negative returns more than positive returns\n",
    "                self.current_capital += self.current_shares * self.current_price\n",
    "                self.current_shares = 0\n",
    "                self.purchase_price = None\n",
    "                self.profit_losses.append(profit_loss)\n",
    "        else:  # Hold\n",
    "            pass\n",
    "\n",
    "        # Check if done\n",
    "        if self.current_date >= self.end_date:\n",
    "            self.done = True\n",
    "\n",
    "        # Update the history\n",
    "        self.history.append((self.current_date, self.current_price, self.current_shares, self.current_capital))\n",
    "\n",
    "        return self.get_state(), reward, self.done, {}\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Return the current state\n",
    "        features = self.data[self.current_date] # numpy array\n",
    "        cur_state =  np.array([self.current_price, self.current_shares, self.current_capital])\n",
    "        return np.concatenate((features, cur_state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, file_path, flush_interval=1000, flush_time=60):\n",
    "        self.file_path = file_path\n",
    "        self.buffer = []\n",
    "        self.flush_interval = flush_interval\n",
    "        self.flush_time = flush_time\n",
    "        self.last_flush_time = time.time()\n",
    "\n",
    "    def log(self, data):\n",
    "        self.buffer.append(data)\n",
    "        if len(self.buffer) >= self.flush_interval or time.time() - self.last_flush_time >= self.flush_time:\n",
    "            self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        with open(self.file_path, 'a') as f:\n",
    "            for data in self.buffer:\n",
    "                f.write(data + '\\n')\n",
    "        self.buffer = []\n",
    "        self.last_flush_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent's policy\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, act_fn=nn.Mish):\n",
    "        super(Policy, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(7, 64),\n",
    "            act_fn(),\n",
    "            nn.Linear(64, 128),\n",
    "            act_fn(),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class BuyPolicy(nn.Module):\n",
    "    def __init__(self, act_fn=nn.Mish):\n",
    "        super(BuyPolicy, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(7, 64),\n",
    "            act_fn(),\n",
    "            nn.Linear(64, 128),\n",
    "            act_fn(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = normalize_data(df, target='close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the en\n",
    "env = TradingEnvironment(data, target=target, initial_capital=10000, start_date=0)\n",
    "\n",
    "# Define the agent's policies\n",
    "policy = Policy()\n",
    "buy_policy = BuyPolicy()\n",
    "\n",
    "# Define the optimizers\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "buy_optimizer = optim.Adam(buy_policy.parameters(), lr=0.01)\n",
    "\n",
    "gamma = 0.99\n",
    "eps_clip = 0.2\n",
    "\n",
    "# Define the logger\n",
    "logger_fname = 'log.txt'\n",
    "if os.path.exists(logger_fname):\n",
    "    os.remove(logger_fname)\n",
    "logger = Logger('log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 9, Total Capital: 1266.1192999999994\n"
     ]
    }
   ],
   "source": [
    "# Train the agent using the PPO algorithm\n",
    "states_dict = {}\n",
    "for i in range(10000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        dist = torch.distributions.Categorical(logits=policy(state))\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            buy_fraction = buy_policy(state).item()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action.item(), buy_fraction)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "            dist = torch.distributions.Categorical(logits=policy(next_state))\n",
    "            next_action = dist.sample()\n",
    "            next_log_prob = dist.log_prob(next_action)\n",
    "            v_next = policy(next_state)[1]\n",
    "\n",
    "        advantage = reward + gamma * (1 - done) * v_next - policy(state)[1]\n",
    "        critic_loss = F.smooth_l1_loss(policy(state)[1], reward + gamma * (1 - done) * v_next)\n",
    "\n",
    "        ratio = torch.exp(log_prob - next_log_prob)\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantage\n",
    "        actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * dist.entropy().mean()\n",
    "\n",
    "        buy_loss = -torch.log(buy_policy(state)[0] + 1e-5) * buy_fraction\n",
    "\n",
    "        loss = actor_loss + critic_loss + buy_loss\n",
    "        optimizer.zero_grad()\n",
    "        buy_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        buy_optimizer.step()\n",
    "\n",
    "        state = next_state.numpy()\n",
    "        \n",
    "        logger.log(f\"Episode: {i}, Action: {action}, Reward: {reward}, Loss: {loss.item()}, Working Capital: {env.current_capital}, Current Shares: {env.current_shares}, Total Capital: {env.current_capital + env.current_shares * env.current_price}, current price: {env.current_price}, buy fraction: {buy_fraction}\")\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"Episode: {i}, Total Capital: {env.current_capital + env.current_shares * env.current_price}\")\n",
    "\n",
    "    states_dict[i] = {\"state\": state, \"total_capital\": env.current_capital + env.current_shares * env.current_price}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0638b84c441d23f3bf1e5bbb68dbbbae5f508c99744b50e7a508082753ac4090"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
